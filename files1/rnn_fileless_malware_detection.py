#!/usr/bin/env python3
"""
RNN-based Fileless Malware Detection System
Trains and compares multiple RNN architectures: Simple RNN, LSTM, GRU, and Bidirectional LSTM
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    confusion_matrix, classification_report, roc_curve, 
    roc_auc_score, precision_recall_curve, f1_score,
    accuracy_score, precision_score, recall_score
)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense, Dropout, SimpleRNN, LSTM, GRU, 
    Bidirectional, BatchNormalization
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)


class FilelessMalwareRNNClassifier:
    """RNN-based classifier for fileless malware detection"""
    
    def __init__(self, output_dir='~/Downloads/files1/outputs'):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        self.models = {}
        self.histories = {}
        self.scaler = StandardScaler()
        
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.feature_names = None
        
    def load_and_combine_data(self, csv_path, excel_path):
        """Load and combine CSV and Excel datasets"""
        print("\n" + "="*70)
        print("LOADING AND COMBINING DATASETS")
        print("="*70)
        
        # Load main dataset
        df_csv = pd.read_csv(csv_path)
        print(f"✓ Loaded {csv_path}: {df_csv.shape}")
        
        # Load additional malware samples
        df_excel = pd.read_excel(excel_path)
        print(f"✓ Loaded {excel_path}: {df_excel.shape}")
        
        # Combine datasets
        df_combined = pd.concat([df_csv, df_excel], ignore_index=True)
        print(f"✓ Combined dataset: {df_combined.shape}")
        
        # Display class distribution
        print(f"\nClass Distribution:")
        print(df_combined['Label'].value_counts())
        print(f"  Benign (1): {(df_combined['Label'] == 1).sum()}")
        print(f"  Malicious (0): {(df_combined['Label'] == 0).sum()}")
        
        return df_combined
    
    def preprocess_data(self, df, test_size=0.3, random_state=42):
        """Preprocess data for RNN training"""
        print("\n" + "="*70)
        print("PREPROCESSING DATA")
        print("="*70)
        
        # Drop Name column
        df = df.drop('Name', axis=1, errors='ignore')
        
        # Separate features and labels
        X = df.drop('Label', axis=1).values
        y = df['Label'].values
        self.feature_names = df.drop('Label', axis=1).columns.tolist()
        
        print(f"✓ Features shape: {X.shape}")
        print(f"✓ Labels shape: {y.shape}")
        print(f"✓ Number of features: {X.shape[1]}")
        
        # Split data
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        print(f"\n✓ Training set: {self.X_train.shape[0]} samples")
        print(f"✓ Testing set: {self.X_test.shape[0]} samples")
        
        # Scale features
        self.X_train = self.scaler.fit_transform(self.X_train)
        self.X_test = self.scaler.transform(self.X_test)
        
        # Reshape for RNN: (samples, timesteps, features)
        # We treat each feature as a timestep
        self.X_train = self.X_train.reshape(self.X_train.shape[0], self.X_train.shape[1], 1)
        self.X_test = self.X_test.reshape(self.X_test.shape[0], self.X_test.shape[1], 1)
        
        print(f"\n✓ Reshaped for RNN:")
        print(f"  Training: {self.X_train.shape}")
        print(f"  Testing: {self.X_test.shape}")
        
        return self
    
    def build_simple_rnn(self, units=64, dropout=0.3):
        """Build Simple RNN model"""
        model = Sequential([
            SimpleRNN(units, return_sequences=True, input_shape=(self.X_train.shape[1], 1)),
            Dropout(dropout),
            SimpleRNN(units // 2, return_sequences=False),
            Dropout(dropout),
            Dense(32, activation='relu'),
            Dropout(dropout),
            Dense(1, activation='sigmoid')
        ])
        return model
    
    def build_lstm(self, units=64, dropout=0.3):
        """Build LSTM model"""
        model = Sequential([
            LSTM(units, return_sequences=True, input_shape=(self.X_train.shape[1], 1)),
            Dropout(dropout),
            LSTM(units // 2, return_sequences=False),
            Dropout(dropout),
            Dense(32, activation='relu'),
            Dropout(dropout),
            Dense(1, activation='sigmoid')
        ])
        return model
    
    def build_gru(self, units=64, dropout=0.3):
        """Build GRU model"""
        model = Sequential([
            GRU(units, return_sequences=True, input_shape=(self.X_train.shape[1], 1)),
            Dropout(dropout),
            GRU(units // 2, return_sequences=False),
            Dropout(dropout),
            Dense(32, activation='relu'),
            Dropout(dropout),
            Dense(1, activation='sigmoid')
        ])
        return model
    
    def build_bidirectional_lstm(self, units=64, dropout=0.3):
        """Build Bidirectional LSTM model"""
        model = Sequential([
            Bidirectional(LSTM(units, return_sequences=True), input_shape=(self.X_train.shape[1], 1)),
            Dropout(dropout),
            Bidirectional(LSTM(units // 2, return_sequences=False)),
            Dropout(dropout),
            Dense(32, activation='relu'),
            Dropout(dropout),
            Dense(1, activation='sigmoid')
        ])
        return model
    
    def train_model(self, model_type='lstm', epochs=100, batch_size=8, 
                   units=64, dropout=0.3, patience=15):
        """Train specified RNN model"""
        print(f"\n{'='*70}")
        print(f"TRAINING {model_type.upper()} MODEL")
        print(f"{'='*70}")
        
        # Build model
        if model_type == 'simple_rnn':
            model = self.build_simple_rnn(units, dropout)
        elif model_type == 'lstm':
            model = self.build_lstm(units, dropout)
        elif model_type == 'gru':
            model = self.build_gru(units, dropout)
        elif model_type == 'bidirectional_lstm':
            model = self.build_bidirectional_lstm(units, dropout)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # Compile model
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
        )
        
        print(f"\n✓ Model architecture:")
        model.summary()
        
        # Callbacks
        early_stop = EarlyStopping(
            monitor='val_loss',
            patience=patience,
            restore_best_weights=True,
            verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=patience // 2,
            min_lr=0.00001,
            verbose=1
        )
        
        # Train model
        print(f"\n✓ Training with {epochs} epochs, batch size {batch_size}...")
        history = model.fit(
            self.X_train, self.y_train,
            validation_split=0.2,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stop, reduce_lr],
            verbose=0  # Set to 1 for detailed progress
        )
        
        # Store model and history
        self.models[model_type] = model
        self.histories[model_type] = history
        
        print(f"\n✓ Training completed!")
        print(f"  Final training accuracy: {history.history['accuracy'][-1]:.4f}")
        print(f"  Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}")
        
        return self
    
    def evaluate_model(self, model_type):
        """Evaluate trained model"""
        print(f"\n{'='*70}")
        print(f"EVALUATING {model_type.upper()} MODEL")
        print(f"{'='*70}")
        
        model = self.models[model_type]
        
        # Predictions
        y_pred_proba = model.predict(self.X_test, verbose=0).flatten()
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # Metrics
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)
        auc = roc_auc_score(self.y_test, y_pred_proba)
        
        print(f"\n✓ Performance Metrics:")
        print(f"  Accuracy:  {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall:    {recall:.4f}")
        print(f"  F1 Score:  {f1:.4f}")
        print(f"  AUC-ROC:   {auc:.4f}")
        
        # Confusion Matrix
        cm = confusion_matrix(self.y_test, y_pred)
        print(f"\n✓ Confusion Matrix:")
        print(cm)
        
        # Save confusion matrix plot
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Malicious', 'Benign'],
                   yticklabels=['Malicious', 'Benign'])
        plt.title(f'Confusion Matrix - {model_type.upper()}', fontsize=14, fontweight='bold')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/confusion_matrix_{model_type}.png', dpi=300)
        plt.close()
        
        # Classification Report
        print(f"\n✓ Classification Report:")
        print(classification_report(self.y_test, y_pred, 
                                   target_names=['Malicious', 'Benign']))
        
        # Store results
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'confusion_matrix': cm
        }
        
        return results
    
    def plot_training_history(self, model_type):
        """Plot training history"""
        history = self.histories[model_type]
        
        # Find precision and recall metric names (TensorFlow may add suffixes)
        precision_key = None
        recall_key = None
        for key in history.history.keys():
            if 'precision' in key.lower() and not key.startswith('val_'):
                precision_key = key
            if 'recall' in key.lower() and not key.startswith('val_'):
                recall_key = key
        
        val_precision_key = f'val_{precision_key}' if precision_key else None
        val_recall_key = f'val_{recall_key}' if recall_key else None
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'Training History - {model_type.upper()}', 
                    fontsize=16, fontweight='bold')
        
        # Accuracy
        axes[0, 0].plot(history.history['accuracy'], label='Training', linewidth=2)
        axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)
        axes[0, 0].set_title('Accuracy', fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Loss
        axes[0, 1].plot(history.history['loss'], label='Training', linewidth=2)
        axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2)
        axes[0, 1].set_title('Loss', fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Precision
        if precision_key and val_precision_key in history.history:
            axes[1, 0].plot(history.history[precision_key], label='Training', linewidth=2)
            axes[1, 0].plot(history.history[val_precision_key], label='Validation', linewidth=2)
            axes[1, 0].set_title('Precision', fontweight='bold')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Precision')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        else:
            axes[1, 0].text(0.5, 0.5, 'Precision metrics not available', 
                           ha='center', va='center', transform=axes[1, 0].transAxes)
        
        # Recall
        if recall_key and val_recall_key in history.history:
            axes[1, 1].plot(history.history[recall_key], label='Training', linewidth=2)
            axes[1, 1].plot(history.history[val_recall_key], label='Validation', linewidth=2)
            axes[1, 1].set_title('Recall', fontweight='bold')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Recall')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'Recall metrics not available', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/training_history_{model_type}.png', dpi=300)
        plt.close()
        print(f"✓ Saved training history plot: training_history_{model_type}.png")
    
    def compare_models(self, results_dict):
        """Compare all trained models"""
        print(f"\n{'='*70}")
        print("MODEL COMPARISON")
        print(f"{'='*70}")
        
        # Create comparison dataframe
        comparison_data = []
        for model_name, results in results_dict.items():
            comparison_data.append({
                'Model': model_name.upper(),
                'Accuracy': results['accuracy'],
                'Precision': results['precision'],
                'Recall': results['recall'],
                'F1 Score': results['f1'],
                'AUC-ROC': results['auc']
            })
        
        df_comparison = pd.DataFrame(comparison_data)
        df_comparison = df_comparison.sort_values('AUC-ROC', ascending=False)
        
        print("\n" + df_comparison.to_string(index=False))
        
        # Save to CSV
        df_comparison.to_csv(f'{self.output_dir}/model_comparison.csv', index=False)
        print(f"\n✓ Saved comparison to: model_comparison.csv")
        
        # Create comparison bar plot
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')
        
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']
        metric_keys = ['accuracy', 'precision', 'recall', 'f1', 'auc']
        colors = plt.cm.Set3(np.linspace(0, 1, len(results_dict)))
        
        for idx, (metric, key) in enumerate(zip(metrics, metric_keys)):
            row = idx // 3
            col = idx % 3
            ax = axes[row, col]
            
            values = [results[key] for results in results_dict.values()]
            bars = ax.bar(range(len(results_dict)), values, color=colors)
            ax.set_xticks(range(len(results_dict)))
            ax.set_xticklabels([m.upper() for m in results_dict.keys()], rotation=45, ha='right')
            ax.set_ylabel(metric)
            ax.set_title(metric, fontweight='bold')
            ax.set_ylim([0, 1])
            ax.grid(True, alpha=0.3, axis='y')
            
            # Add value labels on bars
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.3f}',
                       ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        # Remove empty subplot
        fig.delaxes(axes[1, 2])
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/model_comparison.png', dpi=300)
        plt.close()
        print(f"✓ Saved comparison plot: model_comparison.png")
        
        return df_comparison
    
    def plot_roc_curves(self, results_dict):
        """Plot ROC curves for all models"""
        plt.figure(figsize=(10, 8))
        
        colors = plt.cm.Set2(np.linspace(0, 1, len(results_dict)))
        
        for idx, (model_name, results) in enumerate(results_dict.items()):
            fpr, tpr, _ = roc_curve(self.y_test, results['y_pred_proba'])
            auc = results['auc']
            
            plt.plot(fpr, tpr, color=colors[idx], linewidth=2.5,
                    label=f'{model_name.upper()} (AUC = {auc:.3f})')
        
        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
        plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')
        plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/roc_curves_comparison.png', dpi=300)
        plt.close()
        print(f"✓ Saved ROC curves: roc_curves_comparison.png")


def main():
    """Main execution function"""
    print("\n" + "#"*70)
    print("RNN-BASED FILELESS MALWARE DETECTION SYSTEM")
    print("#"*70)
    
    # Initialize classifier
    classifier = FilelessMalwareRNNClassifier(output_dir='~/Downloads/files1/outputs')
    
    # Load and combine datasets
    df = classifier.load_and_combine_data(
        csv_path='/home/cyber/Downloads/files1/data/newDataset.csv',
        excel_path='/home/cyber/Downloads/files1/data/analysis_of_new_malware_samples_to_be_added_to_the_dataset.xlsx'
    )
    
    # Preprocess data
    classifier.preprocess_data(df, test_size=0.3, random_state=42)
    
    # Train multiple RNN models
    model_types = ['simple_rnn', 'lstm', 'gru', 'bidirectional_lstm']
    results = {}
    
    for model_type in model_types:
        print(f"\n{'#'*70}")
        print(f"Training {model_type.upper()}")
        print(f"{'#'*70}")
        
        # Train model
        classifier.train_model(
            model_type=model_type,
            epochs=100,
            batch_size=8,
            units=64,
            dropout=0.3,
            patience=15
        )
        
        # Evaluate model
        results[model_type] = classifier.evaluate_model(model_type)
        
        # Plot training history
        classifier.plot_training_history(model_type)
    
    # Compare all models
    classifier.compare_models(results)
    
    # Plot ROC curves comparison
    classifier.plot_roc_curves(results)
    
    print(f"\n{'='*70}")
    print("ANALYSIS COMPLETE!")
    print(f"{'='*70}")
    print("\nAll results saved to /mnt/user-data/outputs/")
    print("\nGenerated files:")
    print("  • model_comparison.csv - Performance metrics for all models")
    print("  • model_comparison.png - Bar chart comparing all models")
    print("  • roc_curves_comparison.png - ROC curves for all models")
    print("  • confusion_matrix_*.png - Confusion matrix for each model")
    print("  • training_history_*.png - Training history for each model")


if __name__ == "__main__":
    main()
